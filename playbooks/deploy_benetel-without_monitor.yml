---
# ------------------ Block1 Part 1: Run pos reset asynchronously ------------------
- hosts: webshell
  gather_facts: false
  tasks:
    - name: Run pos reset in parallel for each sopnode to reset
      ansible.builtin.command: >
        ansible-playbook -i inventory/hosts-test.ini playbooks/run_pos.yml --extra-vars "node={{ item }}" -c local
      args:
        chdir: "{{ playbook_dir | dirname }}"
      async: 1800
      poll: 0
      loop: "{{ groups['sopnodes_to_reset'] }}"
      loop_control:
        label: "{{ item }}"
      register: block1_jobs

    - name: Wait for all pos reset jobs
      async_status:
        jid: "{{ item.ansible_job_id }}"
      loop: "{{ block1_jobs.results }}"
      loop_control:
        label: "{{ item.item }}"
      register: block1_results
      until: block1_results.finished
      retries: 100
      delay: 10

# ------------------ Block2: R2Lab tasks ------------------
- hosts: faraday
  gather_facts: false
  tasks:
    # Cleanup and UE setup run synchronously
    - name: Cleanup R2Lab
      include_role:
        name: r2lab/cleanup

    - name: Setup UEs on R2Lab
      include_role:
        name: r2lab/ue/setup

    # Stop all UEs asynchronously (per UE using mini playbook stop_ue.yml)
    - name: Stop all UEs previous connections asynchronously
      ansible.builtin.command: >
        ansible-playbook -i inventory/hosts-test.ini playbooks/stop_ue.yml --extra-vars "ue={{ item }}" -c local
      args:
        chdir: "{{ playbook_dir | dirname }}"
      async: 1800
      poll: 0
      loop: "{{ groups['qhats'] }}"
      loop_control:
        label: "{{ item }}"
      register: stop_ue_jobs

- hosts: localhost
  gather_facts: false
  tasks:
    - name: Wait for all UE stop jobs
      async_status:
        jid: "{{ item.ansible_job_id }}"
      loop: "{{ hostvars['faraday']['stop_ue_jobs'].results }}"
      loop_control:
        label: "{{ item.item }}"
      register: stop_ue_results
      until: stop_ue_results.finished
      retries: 100
      delay: 10

- hosts: faraday
  gather_facts: false
  tasks:
    - name: Power ON RRU
      include_role:
        name: r2lab/rru

# ------------------ Block1 Part 2: Sequential roles ------------------
- hosts: sopnodes
  roles:
    - role: setup/common
    - role: setup/netplan
    - role: setup/containerd
    - role: setup/k8s/k8s_setup
    - role: setup/ovs
    - role: setup/optimization/cpu

- hosts: core_node
  roles:
    - role: setup/k8s/cluster_create
    - role: setup/cni
    - role: setup/storage

- hosts: core_node
  vars:
    ip: "{{ hostvars[groups['ran_node'][0]].ip }}"
  roles:
    - role: setup/gre_tunnel

- hosts: ran_node
  vars:
    ip: "{{ hostvars[groups['core_node'][0]].ip }}"
  roles:
    - role: setup/gre_tunnel

- hosts: k8s_workers
  become: true
  roles:
    - role: setup/k8s/cluster_join
      when: groups['core_node'][0] != groups['ran_node'][0] or groups['core_node'][0] != groups['monitor_node'][0]

- hosts: core_node
  roles:
    - role: 5g/core

- hosts: core_node
  gather_facts: false
  roles:
    - role: scenarios/iperf/upf

- hosts: ran_node
  become: true
  roles:
    - role: setup/k8s/aw2s_specific_tuning
      when: hostvars[groups['faraday'][0]].rru in ['jaguar', 'panther'] and groups['ran_node'][0] in ['sopnode-f1', 'sopnode-f2']
    - role: setup/k8s/aw2s_specific_tuning_f3
      when: hostvars[groups['faraday'][0]].rru in ['jaguar', 'panther'] and groups['ran_node'][0] in ['sopnode-f3']
    - role: setup/k8s/benetel_specific_tuning_f3
      when: hostvars[groups['faraday'][0]].rru in ['benetel1', 'benetel2'] and groups['ran_node'][0] in ['sopnode-f3']

- hosts: monitor_node
  roles:
    - role: monarch/deploy

- hosts: ran_node
  become: true
  roles:
    - role: setup/sriov_operator
    - role: setup/optimization/nic
    - role: benetel/ptp
    - role: benetel/dpdk_install
    - role: benetel/dpdk_config

- hosts: core_node
  roles:
    - role: benetel/sriov_nnp_create

# ------------------ Continue after Block1 and Block2 complete ------------------
- hosts: ran_node
  roles:
    - role: 5g/ran

- hosts: monitor_node
  roles:
    - role: monarch/monitoring

